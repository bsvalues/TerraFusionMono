"""
Model Content Protocol (MCP) LLM Integration

This module provides integration with Large Language Models (LLMs) following 
the MCP standards, enabling advanced AI capabilities within the application.
"""

import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from enum import Enum

from utils.mcp_core import (
    ContentBlock,
    ContentType,
    MCPMessage,
    mcp_function_registry
)
from utils.mcp_agents import Agent, AgentPhase

# Define LLM provider types
class LLMProvider(Enum):
    OPENAI = "openai"  # OpenAI API (GPT models)
    ANTHROPIC = "anthropic"  # Anthropic API (Claude models)
    OLLAMA = "ollama"  # Local Ollama integration
    MOCK = "mock"  # Mock provider for testing without API dependency

class LLMServiceError(Exception):
    """Exception raised when LLM service encounters an error."""
    pass

class MCPLLMService:
    """
    Base LLM service class that implements MCP content standards for AI model integration.
    
    The MCPLLMService serves as an abstract base class for integrating various Large
    Language Model providers (such as OpenAI, Anthropic, etc.) into the MCP framework.
    It defines a standardized interface for interacting with language models while
    maintaining compatibility with the Model Content Protocol.
    
    Key responsibilities:
    - Providing a consistent API for text and structured content generation
    - Converting between MCP content blocks and provider-specific formats
    - Managing API authentication and error handling
    - Supporting function calling capabilities
    
    This class should be extended for specific LLM providers, with concrete
    implementations for the abstract methods.
    """
    
    def __init__(self, provider: LLMProvider, model_name: str, api_key: Optional[str] = None):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key or os.environ.get(f"{provider.value.upper()}_API_KEY")
        
        if not self.api_key and provider != LLMProvider.MOCK and provider != LLMProvider.OLLAMA:
            logging.warning(f"No API key provided for {provider.value}. Only mock operations will be available.")
            
    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate text using the LLM.
        
        Args:
            prompt: The text prompt to send to the LLM
            options: Additional generation options
            
        Returns:
            The generated text
        """
        raise NotImplementedError("Subclasses must implement generate_text")
        
    def generate_content(self, content_blocks: List[ContentBlock], options: Optional[Dict[str, Any]] = None) -> MCPMessage:
        """
        Generate structured content using the LLM following MCP standards.
        
        This method is the primary interface for interacting with language models
        in a way that respects the Model Content Protocol. It accepts a series of
        content blocks (which may contain text, structured data, function calls, etc.)
        and produces a new MCPMessage that contains the model's response.
        
        The implementation should handle:
        1. Converting MCP content blocks to the provider-specific format
        2. Making the appropriate API calls to the model provider
        3. Processing the model's response back into MCP-compatible content blocks
        4. Handling any function calls that might be requested by the model
        
        Args:
            content_blocks: A list of ContentBlock objects representing the input
                           to the language model. These blocks form the context
                           and instructions for the model generation.
            options: Optional dictionary of additional parameters that control the
                    generation behavior, such as temperature, max_tokens, etc.
                    May also include MCP-specific options like function calling settings.
            
        Returns:
            An MCPMessage containing the structured content generated by the model
            as a series of ContentBlock objects.
        """
        raise NotImplementedError("Subclasses must implement generate_content")
        
    def invoke_function(self, function_name: str, parameters: Dict[str, Any]) -> Any:
        """
        Invoke a registered function through LLM reasoning.
        
        Args:
            function_name: The name of the function to invoke
            parameters: The parameters to pass to the function
            
        Returns:
            The function result
        """
        # Get the function from the registry
        try:
            function = mcp_function_registry.get_function(function_name)
            return function(**parameters)
        except Exception as e:
            raise LLMServiceError(f"Error invoking function {function_name}: {str(e)}")
            
    def get_function_definitions(self) -> Dict[str, Dict[str, Any]]:
        """
        Get the function definitions for use with the LLM.
        
        Returns:
            A dictionary of function definitions
        """
        return mcp_function_registry.get_function_catalog()
        
    def convert_blocks_to_prompt(self, content_blocks: List[ContentBlock]) -> str:
        """
        Convert MCP content blocks to a text prompt for non-MCP-aware LLMs.
        
        Args:
            content_blocks: The content blocks to convert
            
        Returns:
            A text prompt
        """
        prompt_parts = []
        
        for block in content_blocks:
            if block.content_type == ContentType.TEXT:
                prompt_parts.append(block.content)
                
            elif block.content_type == ContentType.STRUCTURED_DATA:
                prompt_parts.append(f"Data: {json.dumps(block.content, indent=2)}")
                
            elif block.content_type == ContentType.FUNCTION_CALL:
                function_name = block.content["function"]
                parameters = block.content["parameters"]
                prompt_parts.append(f"Function Call: {function_name}\nParameters: {json.dumps(parameters, indent=2)}")
                
            elif block.content_type == ContentType.FUNCTION_RESPONSE:
                function_name = block.content["function"]
                result = block.content["result"]
                prompt_parts.append(f"Function Response: {function_name}\nResult: {json.dumps(result, indent=2)}")
                
        return "\n\n".join(prompt_parts)
        
    def parse_text_to_content_blocks(self, text: str) -> List[ContentBlock]:
        """
        Parse text response from non-MCP-aware LLM into content blocks.
        This is a best-effort approach for backward compatibility.
        
        Args:
            text: The text response to parse
            
        Returns:
            A list of content blocks
        """
        # For simple responses, just return a text block
        blocks = [ContentBlock(content_type=ContentType.TEXT, content=text)]
        
        # Try to extract structured data or function calls if present
        # This is a simplified approach and might not work for all responses
        try:
            # Check for JSON blocks
            import re
            json_blocks = re.findall(r'```json\n(.*?)\n```', text, re.DOTALL)
            
            for json_str in json_blocks:
                try:
                    data = json.loads(json_str)
                    blocks.append(ContentBlock(
                        content_type=ContentType.STRUCTURED_DATA,
                        content=data
                    ))
                except:
                    pass
                    
            # Check for function call patterns
            function_blocks = re.findall(r'Function Call: (.*?)\nParameters: (.*?)(?:\n\n|$)', text, re.DOTALL)
            
            for function_name, params_str in function_blocks:
                try:
                    parameters = json.loads(params_str)
                    blocks.append(ContentBlock(
                        content_type=ContentType.FUNCTION_CALL,
                        content={
                            "function": function_name.strip(),
                            "parameters": parameters
                        }
                    ))
                except:
                    pass
                    
        except Exception as e:
            logging.warning(f"Error parsing text to content blocks: {str(e)}")
            
        return blocks

class OpenAIService(MCPLLMService):
    """
    OpenAI LLM service implementation following MCP content standards.
    
    This class provides integration with OpenAI's models (like GPT-4, GPT-3.5, etc.) 
    with full support for the Model Content Protocol. It handles the conversion between
    MCP content blocks and OpenAI's message format, and supports advanced features
    like function calling.
    
    The implementation takes advantage of OpenAI's native function calling capabilities
    when available, providing a seamless integration between the MCP framework and
    OpenAI's API.
    """
    
    def __init__(self, model_name: str = "gpt-4", api_key: Optional[str] = None):
        super().__init__(LLMProvider.OPENAI, model_name, api_key)
        
        # Import OpenAI SDK
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key)
        except ImportError:
            logging.error("OpenAI SDK not installed. Please install with 'pip install openai'")
            self.client = None
            
    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:
        """Generate text using OpenAI API."""
        if not self.client:
            raise LLMServiceError("OpenAI SDK not available")
            
        if not self.api_key:
            raise LLMServiceError("OpenAI API key not provided")
            
        options = options or {}
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=options.get("max_tokens", 1024),
                temperature=options.get("temperature", 0.7)
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            raise LLMServiceError(f"OpenAI API error: {str(e)}")
            
    def generate_content(self, content_blocks: List[ContentBlock], options: Optional[Dict[str, Any]] = None) -> MCPMessage:
        """Generate content using OpenAI API following MCP standards."""
        if not self.client:
            raise LLMServiceError("OpenAI SDK not available")
            
        if not self.api_key:
            raise LLMServiceError("OpenAI API key not provided")
            
        options = options or {}
        
        try:
            # Convert content blocks to OpenAI messages format
            messages = []
            
            for block in content_blocks:
                if block.content_type == ContentType.TEXT:
                    messages.append({
                        "role": block.metadata.get("role", "user"),
                        "content": block.content
                    })
                elif block.content_type == ContentType.STRUCTURED_DATA:
                    messages.append({
                        "role": block.metadata.get("role", "user"),
                        "content": json.dumps(block.content)
                    })
                elif block.content_type == ContentType.FUNCTION_CALL:
                    # For older OpenAI API versions that don't support function_call format
                    messages.append({
                        "role": "user",
                        "content": f"Please call function: {block.content['function']} with parameters: {json.dumps(block.content['parameters'])}"
                    })
                    
            # Check if we need to include functions
            functions = None
            function_call = None
            
            if options.get("enable_functions", True):
                function_defs = self.get_function_definitions()
                if function_defs:
                    functions = []
                    for name, metadata in function_defs.items():
                        functions.append({
                            "name": name,
                            "description": metadata.get("description", ""),
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    param_name: {
                                        "type": param_info.get("type", "string"),
                                        "description": param_info.get("description", "")
                                    }
                                    for param_name, param_info in metadata.get("parameters", {}).items()
                                },
                                "required": list(metadata.get("parameters", {}).keys())
                            }
                        })
                    
                    if options.get("auto_function_call", False):
                        function_call = "auto"
                    elif options.get("function_name"):
                        function_call = {"name": options["function_name"]}
                    
            # Make the API call
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                functions=functions,
                function_call=function_call,
                max_tokens=options.get("max_tokens", 1024),
                temperature=options.get("temperature", 0.7)
            )
            
            # Process the response
            assistant_message = response.choices[0].message
            result_blocks = []
            
            # Check for text response
            if assistant_message.content:
                result_blocks.append(ContentBlock(
                    content_type=ContentType.TEXT,
                    content=assistant_message.content,
                    metadata={"role": "assistant"}
                ))
                
            # Check for function call
            if hasattr(assistant_message, 'function_call') and assistant_message.function_call:
                function_name = assistant_message.function_call.name
                arguments = json.loads(assistant_message.function_call.arguments)
                
                result_blocks.append(ContentBlock(
                    content_type=ContentType.FUNCTION_CALL,
                    content={
                        "function": function_name,
                        "parameters": arguments
                    },
                    metadata={"role": "assistant"}
                ))
                
                # If auto execution is enabled, also invoke the function
                if options.get("auto_execute_functions", False):
                    try:
                        result = self.invoke_function(function_name, arguments)
                        
                        result_blocks.append(ContentBlock(
                            content_type=ContentType.FUNCTION_RESPONSE,
                            content={
                                "function": function_name,
                                "result": result
                            },
                            metadata={"auto_executed": True}
                        ))
                    except Exception as e:
                        logging.error(f"Error auto-executing function {function_name}: {str(e)}")
                        
            return MCPMessage(content_blocks=result_blocks)
            
        except Exception as e:
            raise LLMServiceError(f"OpenAI API error: {str(e)}")

class AnthropicService(MCPLLMService):
    """
    Anthropic LLM service implementation following MCP content standards.
    
    This class provides integration with Anthropic's Claude models within the
    MCP framework. It handles the conversion between MCP content blocks and 
    Anthropic's API format, and implements simulated function calling capabilities
    through prompt engineering, as Anthropic may not natively support function calling
    in the same way as OpenAI.
    
    The implementation uses text-based prompt engineering to enable MCP-compliant
    interactions even with models that don't have native MCP support, demonstrating
    how the protocol can be adapted to different LLM architectures.
    """
    
    def __init__(self, model_name: str = "claude-2", api_key: Optional[str] = None):
        super().__init__(LLMProvider.ANTHROPIC, model_name, api_key)
        
        # Import Anthropic SDK if available
        try:
            import anthropic
            self.client = anthropic.Anthropic(api_key=self.api_key)
        except ImportError:
            logging.error("Anthropic SDK not installed. Please install with 'pip install anthropic'")
            self.client = None
            
    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:
        """Generate text using Anthropic API."""
        if not self.client:
            raise LLMServiceError("Anthropic SDK not available")
            
        if not self.api_key:
            raise LLMServiceError("Anthropic API key not provided")
            
        options = options or {}
        
        try:
            response = self.client.completions.create(
                model=self.model_name,
                prompt=f"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}",
                max_tokens_to_sample=options.get("max_tokens", 1024),
                temperature=options.get("temperature", 0.7)
            )
            
            return response.completion
            
        except Exception as e:
            raise LLMServiceError(f"Anthropic API error: {str(e)}")
            
    def generate_content(self, content_blocks: List[ContentBlock], options: Optional[Dict[str, Any]] = None) -> MCPMessage:
        """Generate content using Anthropic API following MCP standards."""
        if not self.client:
            raise LLMServiceError("Anthropic SDK not available")
            
        if not self.api_key:
            raise LLMServiceError("Anthropic API key not provided")
            
        options = options or {}
        
        # Since Anthropic doesn't natively support MCP, convert blocks to prompt text
        prompt = self.convert_blocks_to_prompt(content_blocks)
        
        # Add function definitions if enabled
        if options.get("enable_functions", True):
            function_defs = self.get_function_definitions()
            if function_defs:
                prompt += "\n\nAvailable functions:\n"
                for name, metadata in function_defs.items():
                    prompt += f"- {name}: {metadata.get('description', '')}\n"
                    prompt += f"  Parameters: {json.dumps(metadata.get('parameters', {}), indent=2)}\n\n"
                    
                prompt += "\nYou can call these functions using the format:\n"
                prompt += "Function Call: function_name\n"
                prompt += "Parameters: {\n  \"param1\": \"value1\",\n  \"param2\": \"value2\"\n}\n\n"
                
        try:
            # Generate response
            response_text = self.generate_text(prompt, options)
            
            # Parse the response into content blocks
            result_blocks = self.parse_text_to_content_blocks(response_text)
            
            # Auto-execute functions if enabled
            if options.get("auto_execute_functions", False):
                for block in list(result_blocks):  # Create a copy to avoid modification during iteration
                    if block.content_type == ContentType.FUNCTION_CALL:
                        function_name = block.content["function"]
                        parameters = block.content["parameters"]
                        
                        try:
                            result = self.invoke_function(function_name, parameters)
                            
                            result_blocks.append(ContentBlock(
                                content_type=ContentType.FUNCTION_RESPONSE,
                                content={
                                    "function": function_name,
                                    "result": result
                                },
                                metadata={"auto_executed": True}
                            ))
                        except Exception as e:
                            logging.error(f"Error auto-executing function {function_name}: {str(e)}")
                            
            return MCPMessage(content_blocks=result_blocks)
            
        except Exception as e:
            raise LLMServiceError(f"Anthropic API error: {str(e)}")

class OllamaService(MCPLLMService):
    """
    Ollama LLM service implementation following MCP content standards.
    
    This class provides integration with locally-deployed language models through
    the Ollama framework. It enables using open-source models like Llama, Mistral, 
    and others with the MCP system, supporting self-hosted and air-gapped deployments
    that don't rely on external API services.
    
    The implementation adapts MCP concepts to local model capabilities and uses
    prompt engineering to simulate structured capabilities that might not be
    natively available in all open-source models, demonstrating the protocol's
    flexibility across different model types and hosting arrangements.
    """
    
    def __init__(self, model_name: str = "llama2", base_url: str = "http://localhost:11434"):
        super().__init__(LLMProvider.OLLAMA, model_name)
        self.base_url = base_url
        
        # Set API client
        self.client = None
        
    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:
        """Generate text using Ollama API."""
        options = options or {}
        
        import requests
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "options": {
                        "temperature": options.get("temperature", 0.7),
                        "num_predict": options.get("max_tokens", 1024)
                    }
                }
            )
            
            response.raise_for_status()
            return response.json().get("response", "")
            
        except Exception as e:
            raise LLMServiceError(f"Ollama API error: {str(e)}")
            
    def generate_content(self, content_blocks: List[ContentBlock], options: Optional[Dict[str, Any]] = None) -> MCPMessage:
        """Generate content using Ollama API following MCP standards."""
        options = options or {}
        
        # Since Ollama doesn't natively support MCP, convert blocks to prompt text
        prompt = self.convert_blocks_to_prompt(content_blocks)
        
        # Add function definitions if enabled
        if options.get("enable_functions", True):
            function_defs = self.get_function_definitions()
            if function_defs:
                prompt += "\n\nAvailable functions:\n"
                for name, metadata in function_defs.items():
                    prompt += f"- {name}: {metadata.get('description', '')}\n"
                    prompt += f"  Parameters: {json.dumps(metadata.get('parameters', {}), indent=2)}\n\n"
                    
                prompt += "\nYou can call these functions using the format:\n"
                prompt += "Function Call: function_name\n"
                prompt += "Parameters: {\n  \"param1\": \"value1\",\n  \"param2\": \"value2\"\n}\n\n"
                
        try:
            # Generate response
            response_text = self.generate_text(prompt, options)
            
            # Parse the response into content blocks
            result_blocks = self.parse_text_to_content_blocks(response_text)
            
            # Auto-execute functions if enabled
            if options.get("auto_execute_functions", False):
                for block in list(result_blocks):  # Create a copy to avoid modification during iteration
                    if block.content_type == ContentType.FUNCTION_CALL:
                        function_name = block.content["function"]
                        parameters = block.content["parameters"]
                        
                        try:
                            result = self.invoke_function(function_name, parameters)
                            
                            result_blocks.append(ContentBlock(
                                content_type=ContentType.FUNCTION_RESPONSE,
                                content={
                                    "function": function_name,
                                    "result": result
                                },
                                metadata={"auto_executed": True}
                            ))
                        except Exception as e:
                            logging.error(f"Error auto-executing function {function_name}: {str(e)}")
                            
            return MCPMessage(content_blocks=result_blocks)
            
        except Exception as e:
            raise LLMServiceError(f"Ollama API error: {str(e)}")

class MockLLMService(MCPLLMService):
    """
    Mock LLM service that simulates AI responses for testing and development.
    
    This implementation provides simulated responses without requiring any external
    API access or model deployment. It acts as a lightweight substitute that follows
    the same interface as real LLM integrations, enabling development, testing, and
    demonstrations in environments where actual AI models are unavailable.
    
    The mock service generates deterministic, predictable responses based on input
    patterns, and can simulate function calling capabilities for testing complex
    multi-step agent interactions without incurring API costs or requiring network
    connectivity.
    """
    
    def __init__(self):
        super().__init__(LLMProvider.MOCK, "mock-model")
            
    def generate_text(self, prompt: str, options: Optional[Dict[str, Any]] = None) -> str:
        """Generate mock text response."""
        options = options or {}
        
        # Generate a simple echo response
        return f"MOCK RESPONSE: You said: {prompt[:100]}..."
            
    def generate_content(self, content_blocks: List[ContentBlock], options: Optional[Dict[str, Any]] = None) -> MCPMessage:
        """Generate mock content response following MCP standards."""
        options = options or {}
        result_blocks = []
        
        # Generate a mock text response
        prompt = self.convert_blocks_to_prompt(content_blocks)
        result_blocks.append(ContentBlock(
            content_type=ContentType.TEXT,
            content=f"MOCK RESPONSE: Received {len(content_blocks)} content blocks."
        ))
        
        # If there's a structured data block, echo it back
        for block in content_blocks:
            if block.content_type == ContentType.STRUCTURED_DATA:
                result_blocks.append(ContentBlock(
                    content_type=ContentType.STRUCTURED_DATA,
                    content={"echo": block.content}
                ))
                break
                
        # If function calls are enabled, generate a mock function call
        if options.get("enable_functions", True):
            function_defs = self.get_function_definitions()
            if function_defs and options.get("auto_function_call", False):
                # Pick the first function
                function_name = next(iter(function_defs.keys()))
                parameters = {}
                
                # Generate mock parameters
                for param_name, param_info in function_defs[function_name].get("parameters", {}).items():
                    if param_info.get("type") == "string":
                        parameters[param_name] = "mock_value"
                    elif param_info.get("type") == "number":
                        parameters[param_name] = 42
                    elif param_info.get("type") == "boolean":
                        parameters[param_name] = True
                    else:
                        parameters[param_name] = "mock_value"
                        
                result_blocks.append(ContentBlock(
                    content_type=ContentType.FUNCTION_CALL,
                    content={
                        "function": function_name,
                        "parameters": parameters
                    }
                ))
                
                # Auto-execute if enabled
                if options.get("auto_execute_functions", False):
                    try:
                        result = self.invoke_function(function_name, parameters)
                        
                        result_blocks.append(ContentBlock(
                            content_type=ContentType.FUNCTION_RESPONSE,
                            content={
                                "function": function_name,
                                "result": result
                            },
                            metadata={"auto_executed": True}
                        ))
                    except Exception as e:
                        logging.error(f"Error auto-executing function {function_name}: {str(e)}")
                        
        return MCPMessage(content_blocks=result_blocks)

# LLM-enhanced agent that uses LLMs for reasoning
class LLMAgent(Agent):
    """
    Agent implementation that leverages LLMs for cognitive processes.
    
    The LLMAgent extends the base Agent class by using a language model for its
    core reasoning and response generation capabilities. This provides the agent
    with advanced natural language understanding, contextual reasoning, and
    sophisticated response synthesis.
    
    This class demonstrates how the MCP framework can integrate AI capabilities
    directly into agent architecture, enabling more human-like interactions and
    complex decision-making. The agent follows the perception-reasoning-execution-response
    cycle defined in the base Agent class, but replaces key cognitive components
    with LLM-powered implementations.
    """
    
    def __init__(self, name: str, description: str, llm_service: MCPLLMService):
        super().__init__(name, description)
        self.llm_service = llm_service
        
    def _reasoning_engine(self, perception_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Override the base reasoning engine to use LLM for cognitive processing.
        
        This method replaces the default reasoning implementation with an LLM-powered
        approach that can understand complex contexts, consider nuanced factors, and
        generate sophisticated decision outputs. It converts the perception data into
        a format suitable for LLM processing, generates prompts that guide the model's
        reasoning process, and then extracts structured action plans from the model's
        response.
        
        The implementation includes error handling and fallback to the base reasoning
        engine if the LLM processing encounters any issues.
        
        Args:
            perception_result: Dictionary containing the agent's perception data
            
        Returns:
            Dictionary containing reasoning results, including LLM explanations
            and recommended function calls
        """
        try:
            # Create a structured data block with the perception result
            perception_block = ContentBlock(
                content_type=ContentType.STRUCTURED_DATA,
                content=perception_result
            )
            
            # Create a text block with the reasoning prompt
            prompt_text = f"""
            You are assisting the agent '{self.name}' with reasoning about the following perception result.
            Please analyze the data and determine the best course of action.
            
            Your task is to determine which functions to call and with what parameters.
            
            Return your reasoning in a structured format that can be used for execution.
            """
            
            prompt_block = ContentBlock(
                content_type=ContentType.TEXT,
                content=prompt_text
            )
            
            # Generate content using LLM
            llm_response = self.llm_service.generate_content(
                [prompt_block, perception_block],
                {
                    "enable_functions": True,
                    "auto_function_call": True,
                    "temperature": 0.3  # Lower temperature for more deterministic reasoning
                }
            )
            
            # Extract reasoning from LLM response
            reasoning_result = {"perception_result": perception_result, "llm_reasoning": {}}
            
            for block in llm_response.content_blocks:
                if block.content_type == ContentType.TEXT:
                    reasoning_result["llm_reasoning"]["explanation"] = block.content
                    
                elif block.content_type == ContentType.STRUCTURED_DATA:
                    reasoning_result["llm_reasoning"]["structured_output"] = block.content
                    
                elif block.content_type == ContentType.FUNCTION_CALL:
                    if "function_calls" not in reasoning_result:
                        reasoning_result["function_calls"] = []
                        
                    reasoning_result["function_calls"].append({
                        "function": block.content["function"],
                        "parameters": block.content["parameters"]
                    })
                    
            return reasoning_result
            
        except Exception as e:
            logging.error(f"Error in LLM reasoning: {str(e)}")
            # Fallback to default reasoning
            return super()._reasoning_engine(perception_result)
            
    def _response_generation(self, execution_result: Dict[str, Any]) -> List[ContentBlock]:
        """
        Override the base response generation to use LLM for natural language synthesis.
        
        This method enhances the agent's communication capabilities by leveraging a
        language model to generate nuanced, contextually appropriate responses based
        on the execution results. It transforms technical data into clear, natural
        language explanations, adapts the communication style to the context, and
        structures information in an easily understandable format.
        
        The implementation includes error handling with fallback to the base response
        generation if the LLM processing encounters any issues.
        
        Args:
            execution_result: Dictionary containing the results of function execution
            
        Returns:
            List of ContentBlock objects representing the structured response that
            combines natural language explanations with any relevant structured data
        """
        try:
            # Create a structured data block with the execution result
            execution_block = ContentBlock(
                content_type=ContentType.STRUCTURED_DATA,
                content=execution_result
            )
            
            # Create a text block with the response generation prompt
            prompt_text = f"""
            You are assisting the agent '{self.name}' with generating a response based on the execution result.
            Please synthesize a clear and helpful response that conveys the important information.
            
            Your response should be well-structured and easy to understand.
            """
            
            prompt_block = ContentBlock(
                content_type=ContentType.TEXT,
                content=prompt_text
            )
            
            # Generate content using LLM
            llm_response = self.llm_service.generate_content(
                [prompt_block, execution_block],
                {"temperature": 0.5}
            )
            
            # Return the generated content blocks
            return llm_response.content_blocks
            
        except Exception as e:
            logging.error(f"Error in LLM response generation: {str(e)}")
            # Fallback to default response generation
            return super()._response_generation(execution_result)

# Create a global LLM service instance
def create_llm_service(provider_name: str = None, model_name: str = None, api_key: str = None):
    """
    Create and configure an appropriate LLM service based on available providers.
    
    This factory function implements a cascading fallback strategy to ensure that
    the application can use the best available LLM provider. It tries providers
    in order of capability preference, falling back to simpler options when necessary.
    
    The fallback sequence is:
    1. Use explicitly requested provider if specified
    2. Try OpenAI's models if API key is available
    3. Try Anthropic's models if API key is available
    4. Try local Ollama deployment if available
    5. Fall back to mock implementation for testing/development
    
    This approach ensures maximum flexibility and resilience, allowing the application
    to work in various deployment contexts from cloud-based to fully local environments.
    
    Args:
        provider_name: Optional explicit provider to use ('openai', 'anthropic', 'ollama')
        model_name: Optional specific model name to use with the chosen provider
        api_key: Optional API key to use for authentication with the provider
        
    Returns:
        An initialized MCPLLMService implementation ready for use
    """
    
    # If explicit provider is specified, try to use it
    if provider_name:
        if provider_name.lower() == "openai":
            try:
                return OpenAIService(model_name or "gpt-4", api_key)
            except Exception as e:
                logging.error(f"Failed to initialize OpenAI service: {str(e)}")
                
        elif provider_name.lower() == "anthropic":
            try:
                return AnthropicService(model_name or "claude-2", api_key)
            except Exception as e:
                logging.error(f"Failed to initialize Anthropic service: {str(e)}")
                
        elif provider_name.lower() == "ollama":
            try:
                return OllamaService(model_name or "llama2")
            except Exception as e:
                logging.error(f"Failed to initialize Ollama service: {str(e)}")
                
    # Try to initialize services in order of preference
    # 1. OpenAI (if API key is available)
    if os.environ.get("OPENAI_API_KEY") or api_key:
        try:
            return OpenAIService(model_name or "gpt-4", api_key or os.environ.get("OPENAI_API_KEY"))
        except Exception as e:
            logging.warning(f"Failed to initialize OpenAI service: {str(e)}")
            
    # 2. Anthropic (if API key is available)
    if os.environ.get("ANTHROPIC_API_KEY") or api_key:
        try:
            return AnthropicService(model_name or "claude-2", api_key or os.environ.get("ANTHROPIC_API_KEY"))
        except Exception as e:
            logging.warning(f"Failed to initialize Anthropic service: {str(e)}")
            
    # 3. Local Ollama (if installed)
    try:
        import requests
        requests.get("http://localhost:11434/api/version")
        return OllamaService(model_name or "llama2")
    except Exception as e:
        logging.warning(f"Failed to initialize Ollama service: {str(e)}")
            
    # 4. Fallback to mock
    logging.warning("No LLM service available, using mock service")
    return MockLLMService()

# Initialize a global LLM service
global_llm_service = create_llm_service()

# Create an LLM-enhanced agent
def create_llm_agent(name: str, description: str, llm_service=None):
    """
    Create and configure an intelligent agent with LLM-powered reasoning.
    
    This factory function simplifies the process of creating agents that leverage
    language models for their cognitive capabilities. It handles the configuration
    details and dependency injection, providing a convenient entry point for creating
    sophisticated AI agents.
    
    The function automatically uses the global LLM service if none is explicitly provided,
    enabling a consistent approach to agent creation throughout the application.
    
    Args:
        name: The name identifier for the agent
        description: A description of the agent's purpose and capabilities
        llm_service: Optional specific LLM service to use (defaults to global service)
        
    Returns:
        A fully configured LLMAgent ready for use in MCP workflows
    """
    return LLMAgent(name, description, llm_service or global_llm_service)