ntegration Strategy for MCP-PACS Dev & Production
🎯 Goals Recap:
Isolate development work from live pacs_oltp

Enable safe import/export flows

Keep both inbound and outbound clean (staging, transformations, logs)

Align with existing PACS workflows and avoid accidental writes

📐 Recommended Architecture
rust
Copy
Edit
              ┌──────────────┐         ┌────────────────────┐
              │   External   │         │    MCP Tools (API) │
              │   Systems    │───────▶ │  e.g., get_parcel, │
              └──────────────┘         │  route_appeal, ... │
                    ▲                  └────────────▲───────┘
                    │                               │
                    │                               │
              ┌─────┴──────┐                   ┌─────┴────────┐
              │ Export Stg │◀─────────────────▶│  pacs_training │
              │ (e.g., CSV │    direct dev     │ (acts as oltp) │
              └────────────┘    ↕︎ access       └───────────────┘
                                 for dev only

🧪 Development Environment Configuration
Current Working Database:

Server: jcharrispacs

Dev DB: pacs_training

Production DB (eventual target): pacs_oltp

🧳 Staging Strategy
1. import_staging Tables
For importing data (e.g. mass exemption uploads, OCR records):

sql
Copy
Edit
-- Schema: pacs_training.dbo.import_staging_property_values
CREATE TABLE import_staging_property_values (
  import_id INT IDENTITY(1,1),
  parcel_id VARCHAR(20),
  value_type VARCHAR(50),
  value_amount DECIMAL(18,2),
  effective_date DATE,
  source VARCHAR(255),
  created_at DATETIME DEFAULT GETDATE()
);
2. export_snapshots Tables
Used for:

Exporting appraiser job runs

Archiving MCP output

Manual QA before real PACS insertions

sql
Copy
Edit
-- Schema: pacs_training.dbo.export_snapshot_parcel_timeline
CREATE TABLE export_snapshot_parcel_timeline (
  snapshot_id UNIQUEIDENTIFIER DEFAULT NEWID(),
  parcel_id VARCHAR(20),
  year INT,
  value DECIMAL(18,2),
  extracted_by VARCHAR(50),
  exported_at DATETIME DEFAULT GETDATE()
);
⚙️ Dev Workflow
Local Dev / MCP Testing
All reads/writes go to pacs_training

Export snapshots pulled from pacs_training → staging for validation

Export templates generated by /tools/export_* endpoints

Production Import (Later)
Final data is validated and staged

Manual or automated QA sign-off step

Separate job migrates to pacs_oltp only if QA passed

🔒 Safety Controls (Highly Recommended)
Enforce readonly default access to pacs_oltp

Use parameterized stored procedures only in prod

Log who triggers what export/import, even during dev

Mask or omit sensitive fields when exporting for QA

✅ Example MCP Integration Tool (Staging-Aware)
python
Copy
Edit
@app.post("/tools/validate_exemption_upload")
def validate_exemption_upload(file: UploadFile, db: Session = Depends(get_dev_training_db)):
    """
    Load a CSV file into pacs_training.import_staging_exemptions and validate records.
    """
    raw_data = parse_csv_to_dataframe(file)
    staged = stage_exemptions(raw_data, db)  # Writes to pacs_training.dbo.import_staging_exemptions
    errors = run_exemption_validations(staged)
    return {"staged_count": len(staged), "errors": errors}
🧭 Next Steps
✅ Create staging + snapshot table schemas (I'll scaffold them for you if you want!)

✅ Point all dev tooling to pacs_training

🔄 Automate snapshot exports via /tools/export_parcel_snapshot

🧪 Add a /tools/validate_import endpoint for QA pre-load checks

💾 Version and log every PACS-importable file or script

Would you like me to:

Scaffold the FastAPI endpoints for this export/import toolchain?

Generate the SQL to create these staging/snapshot tables in pacs_training?

Add CLI tooling for snapshot diffing and QA logs?

Let’s build it the right way — audit-first and future-proof.